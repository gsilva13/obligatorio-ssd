# ü§ñ Tienda Alemana - Chat Engine - Backend

Backend para el sistema de chatbot inteligente de Tienda Alemana, construido con Node.js, Express, LangChain y Ollama usando arquitectura RAG (Retrieval-Augmented Generation) para responder preguntas espec√≠ficas sobre productos, precios, ubicaciones y sucursales bas√°ndose en documentos PDF.

## üìã Contexto del Proyecto

Tienda Alemana es una cadena de supermercados que busca mejorar la experiencia de
atenci√≥n a sus clientes, ofreciendo un canal de soporte automatizado mediante un chatbot
inteligente. Con el fin de resolver consultas frecuentes como precios, disponibilidad,
ubicaci√≥n de productos dentro de la tienda, horarios y direcciones de sus sucursales.

### Documentos requeridos:

- **Cat√°logo de Productos**: Art√≠culos, precios y stock disponible
- **Ubicaci√≥n F√≠sica de Productos**: Detalle de qu√© secci√≥n o g√≥ndola corresponde a cada art√≠culo
- **Informaci√≥n General del Local**: Datos de cada sucursal (nombre, horario y direcci√≥n)

## üöÄ Caracter√≠sticas Principales

### RAG (Retrieval-Augmented Generation)

- **Vector Store**: FAISS para almacenamiento y b√∫squeda sem√°ntica de documentos
- **Embeddings**: Modelo `nomic-embed-text` (768 dimensiones) para vectorizaci√≥n
- **Retrieval**: B√∫squeda de similitud sem√°ntica para encontrar documentos relevantes
- **Generation**: Respuestas contextualizadas usando informaci√≥n espec√≠fica de documentos

### LangChain Framework

- **Document Processing**: Carga y divisi√≥n inteligente de PDFs
- **Embeddings Integration**: Integraci√≥n seamless con Ollama embeddings
- **Chain Architecture**: Pipeline robusto para RAG con manejo de errores
- **Memory Management**: Contexto conversacional persistente por sesi√≥n

### Arquitectura Robusta

- **Sistema H√≠brido**: RAG + Chat simple como fallback
- **Diagn√≥stico Completo**: Endpoint de diagn√≥stico para validar toda la pipeline
- **Logging Exhaustivo**: Trazabilidad completa de cada operaci√≥n
- **Rate Limiting**: Protecci√≥n contra abuso (100 req/15min)
- **Validaci√≥n**: Validaci√≥n robusta de datos de entrada
- **Error Handling**: Manejo graceful de errores con fallback autom√°tico

### Tecnolog√≠as

- **Backend**: Node.js + Express
- **LLM**: Ollama (llama3.2:1b para chat, nomic-embed-text para embeddings)
- **Vector Database**: FAISS
- **Framework AI**: LangChain
- **Document Processing**: PDF-Parse con text splitting inteligente

## üìã Prerrequisitos

- **Node.js** (v18 o superior)
- **Ollama** instalado y ejecut√°ndose localmente
- **Modelos Ollama** necesarios:
  - `llama3.2:1b` (modelo de chat)
  - `nomic-embed-text` (modelo de embeddings)
- **Archivos PDF** del negocio (cat√°logo, ubicaciones, sucursales)

## ‚öôÔ∏è Instalaci√≥n

### 1. Instalar dependencias

```bash
cd Backend
npm install
```

### 2. Configurar variables de entorno

```bash
cp .env.example .env
```

Edita el archivo `.env` con tu configuraci√≥n:

```env
PORT=3001
NODE_ENV=development

# Configuraci√≥n Ollama
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_MODEL=llama3.2:1b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Configuraci√≥n de documentos
PDFS_FOLDER=./data/pdfs
VECTOR_STORE_PATH=./data/vector-store
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Rate limiting
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=100
```

### 3. Preparar Ollama

```bash
# Iniciar Ollama (en otra terminal)
ollama serve

# Descargar modelos requeridos
ollama pull llama3.2:1b
ollama pull nomic-embed-text

# Verificar que los modelos est√©n disponibles
ollama list
```

### 4. Preparar documentos PDF

```bash
# Crear estructura de carpetas
mkdir -p data/pdfs data/vector-store

# Coloca los archivos PDF en data/pdfs/:
# - Cat√°logo de Productos.pdf
# - Ubicaci√≥n F√≠sica de Productos.pdf
# - Informaci√≥n de Locales.pdf
```

### 5. Configurar vector store

Una vez colocados los archivos PDFs en la ubicaci√≥n anterior, ejecutar el siguiente comando para crear nuestro vector store con esos PDFs.

```bash
npm run setup-vector-store
```

Este proceso:

1. Lee todos los PDFs de `data/pdfs/`
2. Extrae el texto usando PDF-Parse
3. Divide el texto en chunks de 1000 caracteres con overlap de 200
4. Genera embeddings usando `nomic-embed-text`
5. Crea y guarda el vector store FAISS

### 6. Iniciar servidor

```bash
# Desarrollo con auto-reload
npm run dev

# Producci√≥n
npm start
```

## üß† Implementaci√≥n RAG con LangChain

### Arquitectura RAG

```mermaid
graph TB
    A[Pregunta Usuario] --> B[Embedding de la pregunta]
    B --> C[B√∫squeda en Vector Store]
    C --> D[Documentos relevantes]
    D --> E[Prompt con contexto]
    E --> F[LLM Ollama]
    F --> G[Respuesta contextualizada]

    H[PDFs] --> I[Text Splitting]
    I --> J[Embeddings]
    J --> K[Vector Store FAISS]
    K --> C
```

### Componentes LangChain Utilizados

#### 1. Document Loading & Processing

```javascript
// Carga y procesamiento de PDFs
const PDFParse = require("pdf-parse");
const { RecursiveCharacterTextSplitter } = require("langchain/text_splitter");

// Configuraci√≥n del text splitter
this.textSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000,
  chunkOverlap: 200,
  separators: ["\n\n", "\n", ". ", " ", ""],
});
```

#### 2. Embeddings

```javascript
// Embeddings con Ollama
const { OllamaEmbeddings } = require("@langchain/community/embeddings/ollama");

this.embeddings = new OllamaEmbeddings({
  baseUrl: "http://127.0.0.1:11434",
  model: "nomic-embed-text", // 768 dimensiones
});
```

#### 3. Vector Store

```javascript
// Vector store FAISS
const { FaissStore } = require("@langchain/community/vectorstores/faiss");

// Crear vector store desde documentos
const vectorStore = await FaissStore.fromTexts(
  texts,
  metadatas,
  this.embeddings
);

// Guardar para persistencia
await vectorStore.save(vectorStorePath);
```

#### 4. Retrieval Chain

```javascript
// Configuraci√≥n del retriever
const retriever = this.vectorStore.asRetriever({
  k: 5, // Top 5 documentos m√°s relevantes
  searchType: "similarity",
});

// Chain RAG completa
this.chain = RunnableSequence.from([
  {
    context: retriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
    conversationHistory: (input) => getConversationContext(input.userId),
  },
  promptTemplate,
  this.ollama,
  new StringOutputParser(),
]);
```

#### 5. Prompt Engineering

```javascript
const promptTemplate = PromptTemplate.fromTemplate(`
Eres un asistente de Tienda Alemana, una cadena de supermercados. 
Tu trabajo es ayudar a los clientes respondiendo preguntas sobre:
- Productos disponibles, precios y stock
- Ubicaci√≥n de productos dentro de la tienda (g√≥ndolas y secciones)
- Informaci√≥n de sucursales (horarios, direcciones)

Contexto relevante de documentos:
{context}

{conversationHistory}

Pregunta actual del cliente: {question}

Instrucciones:
- Responde de manera amigable y profesional
- Mant√©n coherencia con la conversaci√≥n anterior si existe
- Si la informaci√≥n no est√° disponible en el contexto, dilo claramente
- Para preguntas sobre precios, incluye el s√≠mbolo $ y el valor exacto
- Para ubicaciones, menciona la g√≥ndola o secci√≥n espec√≠fica
- Para horarios, menciona d√≠as y horas completas

Respuesta:
`);
```

### Sistema H√≠brido con Fallback

El sistema implementa un enfoque h√≠brido robusto:

1. **RAG Principal**: Intenta usar documentos espec√≠ficos
2. **Chat con Contexto**: Si RAG falla, usa contexto conversacional
3. **Fallback Simple**: Chat b√°sico sin contexto como √∫ltimo recurso

```javascript
async hybridChat(message, sessionId) {
  // 1. Intentar RAG con documentos espec√≠ficos
  try {
    const docs = await this.vectorStore.similaritySearch(message, 3);
    if (docs.length > 0) {
      // Generar respuesta con contexto de documentos
      return await this.generateRAGResponse(message, docs, sessionId);
    }
  } catch (ragError) {
    logger.error("RAG fall√≥, usando contexto conversacional", ragError);
  }

  // 2. Fallback con contexto conversacional
  return await this.generateContextualResponse(message, sessionId);
}
```

## üìö API Documentation

### Base URL

```
http://localhost:3001/api
```

### Authentication

No authentication required for current implementation.

### Rate Limiting

- **Limit**: 100 requests per 15 minutes per IP
- **Headers**: `X-RateLimit-Limit`, `X-RateLimit-Remaining`, `X-RateLimit-Reset`

---

## ü§ñ Chat Endpoints

### POST /api/chat

Enviar un mensaje al chatbot y recibir una respuesta contextualizada.

**Request Body:**

```json
{
  "message": "¬øCu√°nto cuesta la leche de 1 litro?",
  "userId": "user123" // Opcional, para mantener contexto conversacional
}
```

**Response:**

```json
{
  "success": true,
  "data": {
    "response": "La Leche Entera de 1 litro cuesta $45 y tenemos 120 unidades disponibles. La puedes encontrar en la G√≥ndola de L√°cteos (G√≥ndola 2).",
    "userId": "user123",
    "timestamp": "2025-06-30T01:15:30.123Z",
    "ragUsed": true
  }
}
```

**Error Response:**

```json
{
  "success": false,
  "error": "Error procesando la consulta",
  "message": "Detailed error message",
  "timestamp": "2025-06-30T01:15:30.123Z"
}
```

### POST /api/chat/search

Buscar documentos relevantes sin generar respuesta del chatbot.

**Request Body:**

```json
{
  "query": "precios de productos l√°cteos",
  "limit": 5 // Opcional, default: 5
}
```

**Response:**

```json
{
  "success": true,
  "data": {
    "documents": [
      {
        "content": "La Leche Entera de 1 litro se vende a $45 y tenemos 120 unidades...",
        "metadata": {
          "source": "Cat√°logo de Productos.pdf",
          "type": "catalog",
          "chunkIndex": 0,
          "chunkSize": 984
        },
        "score": 0.85
      }
    ],
    "totalFound": 3,
    "query": "precios de productos l√°cteos"
  }
}
```

### GET /api/chat/health

Verificar estado del servicio de chat.

**Response:**

```json
{
  "success": true,
  "data": {
    "status": "healthy",
    "initialized": true,
    "ollama": "connected",
    "vectorStore": "loaded",
    "timestamp": "2025-06-30T01:15:30.123Z"
  }
}
```

### GET /api/chat/diagnose

Ejecutar diagn√≥stico completo del sistema RAG.

**Response:**

```json
{
  "success": true,
  "data": {
    "diagnosis": {
      "timestamp": "2025-06-30T01:15:30.123Z",
      "initialized": true,
      "components": {
        "ollama": true,
        "embeddings": true,
        "vectorStore": true,
        "chain": true
      },
      "tests": {
        "ollamaConnection": {
          "success": true,
          "data": {
            "status": "connected",
            "model": "llama3.2:1b",
            "version": "0.9.0"
          }
        },
        "embeddings": {
          "success": true,
          "dimensions": 768,
          "sampleValues": [0.48, 0.62, -3.83, -0.12, 1.84]
        },
        "vectorSearch": {
          "success": true,
          "documentsFound": 2,
          "sampleContent": ["texto del documento...", "otro texto..."]
        },
        "endToEndRAG": {
          "success": true,
          "ragUsed": true,
          "responseLength": 245
        }
      }
    }
  }
}
```

### POST /api/chat/reinitialize

Reinicializar el servicio de chat (√∫til despu√©s de actualizar documentos).

**Response:**

```json
{
  "success": true,
  "data": {
    "message": "Chat service reinitialized successfully",
    "timestamp": "2025-06-30T01:15:30.123Z"
  }
}
```

---

## üìÑ Document Management Endpoints

### POST /api/documents/upload

Subir y procesar nuevos archivos PDF.

**Request:**

- **Content-Type**: `multipart/form-data`
- **Field**: `pdfs` (puede ser m√∫ltiples archivos)

**Response:**

```json
{
  "success": true,
  "data": {
    "message": "Documents processed successfully",
    "filesProcessed": 2,
    "documentsCreated": 15,
    "vectorStoreUpdated": true,
    "files": ["catalogo.pdf", "ubicaciones.pdf"]
  }
}
```

### POST /api/documents/setup

Configurar vector store desde PDFs por defecto en `data/pdfs/`.

**Response:**

```json
{
  "success": true,
  "data": {
    "message": "Vector store configured successfully",
    "documentsProcessed": 4,
    "pdfFiles": ["Cat√°logo de Productos.pdf", "Informaci√≥n de Locales.pdf"],
    "vectorStorePath": "./data/vector-store"
  }
}
```

### GET /api/documents/status

Obtener estado de los documentos procesados.

**Response:**

```json
{
  "success": true,
  "data": {
    "vectorStoreExists": true,
    "vectorStorePath": "./data/vector-store",
    "lastUpdated": "2025-06-30T01:05:02.695Z",
    "documentCount": 4,
    "sourceFiles": [
      {
        "name": "Cat√°logo de Productos.pdf",
        "type": "catalog",
        "processedAt": "2025-06-30T01:05:02.695Z",
        "chunks": 2
      }
    ]
  }
}
```

### DELETE /api/documents/reset

Reiniciar vector store (eliminar todos los documentos).

**Response:**

```json
{
  "success": true,
  "data": {
    "message": "Vector store reset successfully",
    "timestamp": "2025-06-30T01:15:30.123Z"
  }
}
```

---

## üè• Health Check Endpoints

### GET /api/health

Estado general del servidor.

**Response:**

```json
{
  "success": true,
  "data": {
    "status": "healthy",
    "timestamp": "2025-06-30T01:15:30.123Z",
    "uptime": 3600,
    "environment": "development",
    "version": "1.0.0"
  }
}
```

### GET /api/health/detailed

Estado detallado de todos los servicios.

**Response:**

```json
{
  "success": true,
  "data": {
    "server": {
      "status": "healthy",
      "uptime": 3600,
      "memory": {
        "used": "145.2 MB",
        "total": "2.1 GB"
      }
    },
    "ollama": {
      "status": "connected",
      "baseUrl": "http://127.0.0.1:11434",
      "model": "llama3.2:1b",
      "embeddings": "nomic-embed-text"
    },
    "vectorStore": {
      "status": "loaded",
      "path": "./data/vector-store",
      "documentCount": 4
    },
    "chatService": {
      "status": "initialized",
      "initialized": true
    }
  }
}
```

---

## üìù Request/Response Examples

### Ejemplo 1: Consulta de Precio

```bash
curl -X POST http://localhost:3001/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "¬øCu√°nto cuesta el jugo de naranja?",
    "userId": "customer_001"
  }'
```

**Respuesta esperada:**

```json
{
  "success": true,
  "data": {
    "response": "El Jugo de Naranja de 1 litro cuesta $89 y tenemos 90 unidades disponibles. Lo puedes encontrar en la G√≥ndola de Bebidas.",
    "userId": "customer_001",
    "timestamp": "2025-06-30T01:15:30.123Z",
    "ragUsed": true
  }
}
```

### Ejemplo 2: Consulta de Ubicaci√≥n

```bash
curl -X POST http://localhost:3001/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "¬øD√≥nde encuentro el yogur de frutilla?",
    "userId": "customer_001"
  }'
```

### Ejemplo 3: Subir Documentos

```bash
curl -X POST http://localhost:3001/api/documents/upload \
  -F "pdfs=@./catalogo-actualizado.pdf" \
  -F "pdfs=@./nuevas-ubicaciones.pdf"
```

### Ejemplo 4: Buscar Documentos

```bash
curl -X POST http://localhost:3001/api/chat/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "productos con descuento",
    "limit": 3
  }'
```

---

## ‚ö†Ô∏è Error Codes

| Code | Description           | Example                      |
| ---- | --------------------- | ---------------------------- |
| 400  | Bad Request           | Missing required fields      |
| 429  | Too Many Requests     | Rate limit exceeded          |
| 500  | Internal Server Error | Ollama connection failed     |
| 503  | Service Unavailable   | Chat service not initialized |

### Error Response Format

```json
{
  "success": false,
  "error": "Error description",
  "message": "Detailed error message",
  "code": "ERROR_CODE",
  "timestamp": "2025-06-30T01:15:30.123Z"
}
```

## üèóÔ∏è Arquitectura del Sistema

### Estructura de Archivos

```
Backend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ server.js                    # Servidor Express principal
‚îÇ   ‚îú‚îÄ‚îÄ routes/                      # Definici√≥n de endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.js                 # Rutas del chatbot (/api/chat/*)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ documents.js            # Gesti√≥n de documentos (/api/documents/*)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ health.js               # Health checks (/api/health/*)
‚îÇ   ‚îú‚îÄ‚îÄ services/                    # L√≥gica de negocio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chatService.js          # Servicio principal RAG + Chat
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ documentService.js      # Procesamiento de PDFs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sessionService.js       # Gesti√≥n de sesiones de usuario
‚îÇ   ‚îú‚îÄ‚îÄ middleware/                  # Middlewares Express
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ errorHandler.js         # Manejo centralizado de errores
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rateLimiter.js          # Rate limiting por IP
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validation.js           # Validaci√≥n de requests
‚îÇ   ‚îú‚îÄ‚îÄ utils/                       # Utilidades
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.js               # Sistema de logging (Winston)
‚îÇ   ‚îî‚îÄ‚îÄ scripts/                     # Scripts de configuraci√≥n
‚îÇ       ‚îî‚îÄ‚îÄ setupVectorStore.js     # Setup inicial del vector store
‚îú‚îÄ‚îÄ data/                           # Datos persistentes
‚îÇ   ‚îú‚îÄ‚îÄ pdfs/                       # Archivos PDF fuente
‚îÇ   ‚îî‚îÄ‚îÄ vector-store/               # Vector store FAISS
‚îú‚îÄ‚îÄ logs/                           # Archivos de log
‚îÇ   ‚îú‚îÄ‚îÄ app.log                     # Logs generales
‚îÇ   ‚îú‚îÄ‚îÄ error.log                   # Solo errores
‚îÇ   ‚îî‚îÄ‚îÄ exceptions.log              # Excepciones no capturadas
‚îú‚îÄ‚îÄ .env                            # Variables de entorno
‚îú‚îÄ‚îÄ package.json                    # Dependencias y scripts
‚îî‚îÄ‚îÄ README.md                       # Esta documentaci√≥n
```

### Flujo de Datos RAG

1. **Ingesta de Documentos**:

   ```
   PDF Files ‚Üí PDF-Parse ‚Üí Text Chunks ‚Üí Embeddings ‚Üí FAISS Vector Store
   ```

2. **Consulta del Usuario**:

   ```
   User Query ‚Üí Embedding ‚Üí Vector Search ‚Üí Relevant Docs ‚Üí LLM ‚Üí Response
   ```

3. **Gesti√≥n de Sesiones**:
   ```
   User ID ‚Üí Session Store ‚Üí Conversation History ‚Üí Context for LLM
   ```

### Componentes Clave

#### ChatService (`src/services/chatService.js`)

- **Prop√≥sito**: Servicio principal que orquesta todo el pipeline RAG
- **Responsabilidades**:

  - Inicializaci√≥n de Ollama (chat + embeddings)
  - Carga y gesti√≥n del vector store FAISS
  - Ejecuci√≥n del pipeline RAG h√≠brido
  - Diagn√≥stico y health checks
  - Manejo de contexto conversacional

- **M√©todos principales**:
  ```javascript
  async initialize()           // Inicializa todos los componentes
  async chat(message, userId)  // Endpoint principal de chat
  async hybridChat()          // L√≥gica RAG h√≠brida
  async diagnoseRAG()         // Diagn√≥stico completo
  async loadVectorStore()     // Carga del vector store
  ```

#### DocumentService (`src/services/documentService.js`)

- **Prop√≥sito**: Procesamiento y gesti√≥n de documentos PDF
- **Responsabilidades**:

  - Extracci√≥n de texto de PDFs
  - Divisi√≥n inteligente en chunks
  - Creaci√≥n y actualizaci√≥n del vector store
  - Categorizaci√≥n autom√°tica de documentos

- **Flujo de procesamiento**:
  ```javascript
  PDF ‚Üí extractTextFromPDF() ‚Üí splitIntoChunks() ‚Üí
  categorizeDocument() ‚Üí createVectorStore() ‚Üí save()
  ```

#### SessionService (`src/services/sessionService.js`)

- **Prop√≥sito**: Gesti√≥n de sesiones y contexto conversacional
- **Caracter√≠sticas**:
  - Almacenamiento en memoria de conversaciones
  - L√≠mite de mensajes por sesi√≥n (10 por defecto)
  - Contexto formateado para el LLM
  - Auto-cleanup de sesiones antigas

### Integraciones LangChain

#### 1. Document Loaders

```javascript
// No usa loaders built-in, procesamiento custom con PDF-Parse
// para mayor control sobre metadata y categorizaci√≥n
const data = await PDFParse(pdfBuffer);
const chunks = await this.textSplitter.splitText(data.text);
```

#### 2. Text Splitters

```javascript
const { RecursiveCharacterTextSplitter } = require("langchain/text_splitter");

this.textSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000, // Tama√±o de cada chunk
  chunkOverlap: 200, // Overlap entre chunks
  separators: ["\n\n", "\n", ". ", " ", ""], // Separadores inteligentes
});
```

#### 3. Embeddings

```javascript
const { OllamaEmbeddings } = require("@langchain/community/embeddings/ollama");

// Configuraci√≥n de embeddings con modelo espec√≠fico
this.embeddings = new OllamaEmbeddings({
  baseUrl: "http://127.0.0.1:11434",
  model: "nomic-embed-text", // Modelo optimizado para espa√±ol
});
```

#### 4. Vector Stores

```javascript
const { FaissStore } = require("@langchain/community/vectorstores/faiss");

// Creaci√≥n desde textos
const vectorStore = await FaissStore.fromTexts(
  texts, // Array de contenidos
  metadatas, // Array de metadatos paralelo
  embeddings // Instancia de embeddings
);

// Persistencia
await vectorStore.save(path);
const loadedStore = await FaissStore.load(path, embeddings);
```

#### 5. Retrievers

```javascript
// Configuraci√≥n del retriever con par√°metros optimizados
const retriever = this.vectorStore.asRetriever({
  k: 5, // Top-K documentos m√°s relevantes
  searchType: "similarity", // Tipo de b√∫squeda
  searchKwargs: {
    scoreThreshold: 0.7, // Threshold de relevancia
  },
});
```

#### 6. Chains y Runnables

```javascript
const {
  RunnableSequence,
  RunnablePassthrough,
} = require("@langchain/core/runnables");

// Chain completa RAG
this.chain = RunnableSequence.from([
  {
    context: retriever.pipe(formatDocs), // Contexto de documentos
    question: new RunnablePassthrough(), // Pregunta original
    conversationHistory: (input) => {
      // Historia conversacional
      return sessionService.getConversationContext(input.userId);
    },
  },
  promptTemplate, // Template del prompt
  this.ollama, // Modelo de chat
  new StringOutputParser(), // Parser de salida
]);
```

### Prompts Engineering

#### Prompt Principal RAG

```javascript
const promptTemplate = PromptTemplate.fromTemplate(`
Eres un asistente de Tienda Alemana, una cadena de supermercados. 
Tu trabajo es ayudar a los clientes respondiendo preguntas sobre:
- Productos disponibles, precios y stock
- Ubicaci√≥n de productos dentro de la tienda (g√≥ndolas y secciones)
- Informaci√≥n de sucursales (horarios, direcciones)

Contexto relevante de documentos:
{context}

{conversationHistory}

Pregunta actual del cliente: {question}

Instrucciones:
- Responde de manera amigable y profesional
- Mant√©n coherencia con la conversaci√≥n anterior si existe
- Si la informaci√≥n no est√° disponible en el contexto, dilo claramente
- Para preguntas sobre precios, incluye el s√≠mbolo $ y el valor exacto
- Para ubicaciones, menciona la g√≥ndola o secci√≥n espec√≠fica
- Para horarios, menciona d√≠as y horas completas
- Si el cliente hace referencia a algo mencionado anteriormente, √∫salo como contexto
- Si no puedes responder completamente, sugiere contactar con el personal de la tienda

Respuesta:
`);
```

#### Caracter√≠sticas del Prompt

- **Contexto espec√≠fico**: Define claramente el rol y dominio
- **Instrucciones detalladas**: Comportamiento esperado para diferentes tipos de consultas
- **Variables din√°micas**: Contexto de documentos, historia conversacional, pregunta actual
- **Fallback guidance**: Instrucciones para casos sin informaci√≥n suficiente

### Logging y Monitoreo

#### Configuraci√≥n Winston

```javascript
// logger.js - Sistema de logging multi-level
const winston = require("winston");

const logger = winston.createLogger({
  level: "debug",
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json()
  ),
  transports: [
    new winston.transports.File({ filename: "logs/error.log", level: "error" }),
    new winston.transports.File({ filename: "logs/app.log" }),
    new winston.transports.Console({
      format: winston.format.combine(
        winston.format.colorize(),
        winston.format.simple()
      ),
    }),
  ],
});
```

#### Logs Estructurados RAG

- **üöÄ Inicializaci√≥n**: Estados de componentes
- **üîç B√∫squeda**: Query, documentos encontrados, similitud
- **ü§ñ Generaci√≥n**: Prompts, respuestas, m√©tricas
- **‚ùå Errores**: Stack traces completos, contexto de fallo
- **üìä M√©tricas**: Tiempos de respuesta, uso de RAG vs fallback

## üéØ Preguntas de ejemplo para evaluar el chatbot

### Cat√°logo de productos

- "¬øCu√°nto cuesta la Leche Entera de 1 litro?"
- "¬øCu√°ntas unidades hay disponibles de Queso Muzzarella?"
- "¬øQu√© productos est√°n por debajo de $80?"
- "¬øTienen stock de At√∫n en lata?"
- "¬øCu√°l es el precio del detergente con aroma a lim√≥n?"

### Ubicaci√≥n de productos

- "¬øEn qu√© g√≥ndola puedo encontrar el Yogur de frutilla?"
- "¬øD√≥nde est√° ubicado el arroz de 1 kg dentro del supermercado?"
- "¬øEn qu√© secci√≥n se encuentran los fideos?"
- "¬øD√≥nde puedo encontrar el jugo de naranja?"
- "¬øQu√© productos est√°n en la g√≥ndola 5?"

### Informaci√≥n de sucursales

- "¬øCu√°l es la direcci√≥n de la sucursal de Tienda Alemana en Pocitos?"
- "¬øHasta qu√© hora abre Tienda Alemana Central?"
- "¬øCu√°les son los horarios de atenci√≥n de la sucursal de Carrasco?"
- "¬øQu√© d√≠as abre la tienda ubicada en Av. Brasil?"
- "¬øTienen alguna tienda abierta los domingos?"

## üîç Logs y Monitoreo

### Estructura de Logs

```
logs/
‚îú‚îÄ‚îÄ app.log           # Logs generales (INFO, DEBUG, WARN)
‚îú‚îÄ‚îÄ error.log         # Solo errores (ERROR nivel)
‚îî‚îÄ‚îÄ exceptions.log    # Excepciones no capturadas
```

### Niveles de Log

- **ÔøΩ DEBUG**: Detalles t√©cnicos, valores de variables, flujo interno
- **‚ÑπÔ∏è INFO**: Eventos importantes, inicializaciones, respuestas exitosas
- **‚ö†Ô∏è WARN**: Situaciones an√≥malas no cr√≠ticas, fallbacks
- **‚ùå ERROR**: Errores que impiden operaciones, excepciones

### Logs RAG Espec√≠ficos

```bash
# Inicializaci√≥n
üöÄ Inicializando ChatService...
‚úÖ ChatOllama configurado
üßÆ Configurando OllamaEmbeddings...
üìö Cargando vector store...

# Procesamiento de consulta
üîç Iniciando b√∫squeda RAG para: "¬øcu√°nto cuesta la leche?"
üìÑ Documentos encontrados: 3
üìö Contexto RAG generado: 1205 caracteres
‚úÖ Respuesta generada exitosamente

# Diagn√≥stico
üîß Iniciando diagn√≥stico completo de RAG...
‚úÖ Test Ollama: PASSED
‚úÖ Test Embeddings: PASSED (768 dims)
‚úÖ Test Vector Search: PASSED (2 docs)
üéØ Diagn√≥stico completado: 4/4 tests passed
```

## üõ†Ô∏è Desarrollo y Comandos

### Scripts NPM Disponibles

```bash
npm run dev              # Desarrollo con nodemon (auto-reload)
npm start               # Producci√≥n
npm run setup-vector-store  # Configurar vector store inicial
```

### Variables de Entorno Completas

```env
# Servidor
PORT=3001
NODE_ENV=development

# Ollama Configuration
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_MODEL=llama3.2:1b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Document Processing
PDFS_FOLDER=./data/pdfs
VECTOR_STORE_PATH=./data/vector-store
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Rate Limiting
RATE_LIMIT_WINDOW_MS=900000      # 15 minutos
RATE_LIMIT_MAX_REQUESTS=100      # 100 requests por ventana

# Session Management
MAX_MESSAGES_PER_SESSION=10      # M√°ximo mensajes en contexto
SESSION_TIMEOUT_MINUTES=30       # Timeout de sesi√≥n inactiva

# Logging
LOG_LEVEL=debug                  # debug, info, warn, error
LOG_TO_FILE=true
LOG_TO_CONSOLE=true
```

### Estructura de Configuraci√≥n

#### Ollama Setup

```bash
# 1. Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Iniciar servicio
ollama serve

# 3. Descargar modelos requeridos
ollama pull llama3.2:1b      # Modelo de chat (1.3GB)
ollama pull nomic-embed-text # Modelo de embeddings (274MB)

# 4. Verificar instalaci√≥n
ollama list
```

#### Preparaci√≥n de Datos

```bash
# Crear estructura de directorios
mkdir -p data/{pdfs,vector-store} logs

# Verificar permisos
chmod 755 data/
chmod 755 logs/

# Copiar PDFs de ejemplo
cp sample-pdfs/* data/pdfs/
```

## üß™ Testing y Validaci√≥n

### Testing Manual API

#### 1. Health Check

```bash
curl http://localhost:3001/api/health
```

#### 2. Diagn√≥stico RAG

```bash
curl http://localhost:3001/api/chat/diagnose
```

#### 3. Chat Test

```bash
curl -X POST http://localhost:3001/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "¬øQu√© productos tienes disponibles?"}'
```

#### 4. Document Upload

```bash
curl -X POST http://localhost:3001/api/documents/upload \
  -F "pdfs=@test-catalog.pdf"
```

### Casos de Prueba Recomendados

#### Consultas de Productos

```bash
# Precios espec√≠ficos
"¬øCu√°nto cuesta la leche de 1 litro?"
"¬øCu√°l es el precio del yogur de frutilla?"

# Stock y disponibilidad
"¬øTienen stock de at√∫n en lata?"
"¬øCu√°ntas unidades hay de queso muzzarella?"

# Comparaci√≥n de precios
"¬øQu√© productos cuestan menos de $50?"
```

#### Consultas de Ubicaci√≥n

```bash
# Ubicaci√≥n espec√≠fica
"¬øD√≥nde encuentro el arroz?"
"¬øEn qu√© g√≥ndola est√° el detergente?"

# Navegaci√≥n por secciones
"¬øQu√© productos hay en la g√≥ndola 3?"
"¬øQu√© hay en la secci√≥n de l√°cteos?"
```

#### Consultas de Sucursales

```bash
# Informaci√≥n de horarios
"¬øA qu√© hora abre la sucursal de Pocitos?"
"¬øQu√© d√≠as est√°n abiertos?"

# Direcciones
"¬øCu√°l es la direcci√≥n de la tienda central?"
```

### M√©tricas de Performance

#### Tiempos Esperados

- **Inicializaci√≥n**: 2-5 segundos
- **Consulta simple**: 1-3 segundos
- **Consulta RAG**: 2-5 segundos
- **Upload documento**: 5-15 segundos (depende del tama√±o)

#### Recursos del Sistema

- **RAM**: ~200-500MB (depende del modelo Ollama)
- **Almacenamiento**: ~50MB para vector store t√≠pico
- **CPU**: Picos durante generaci√≥n de embeddings

## üö® Soluci√≥n de Problemas

### Problemas Comunes y Soluciones

#### 1. Error: Cannot connect to Ollama

```bash
# S√≠ntomas
‚ùå Error conectando con Ollama: connect ECONNREFUSED 127.0.0.1:11434

# Soluciones
# Verificar que Ollama est√© ejecut√°ndose
ollama serve

# Verificar que los modelos est√©n descargados
ollama list

# Verificar conectividad
curl http://localhost:11434/api/version
```

#### 2. Error: No PDF files found

```bash
# S√≠ntomas
‚ùå No se encontraron archivos PDF para procesar

# Soluciones
# Crear carpeta y agregar PDFs
mkdir -p data/pdfs
cp your-pdfs/* data/pdfs/

# Verificar archivos
ls -la data/pdfs/
```

#### 3. Error: Vector store incompatible

```bash
# S√≠ntomas
‚ùå Query vector must have the same length as the number of dimensions (2048)

# Soluciones
# Eliminar vector store incompatible
rm -rf data/vector-store

# Verificar configuraci√≥n de embedding model
grep OLLAMA_EMBEDDING_MODEL .env

# Recrear vector store
npm run setup-vector-store
```

#### 4. Error: Embedding dimensions mismatch

```bash
# S√≠ntomas
‚ùå Embeddings funcionando, dimensiones: 2048 (esperado: 768)

# Causa
# Configuraci√≥n incorrecta en documentService.js usando OLLAMA_MODEL
# en lugar de OLLAMA_EMBEDDING_MODEL

# Soluci√≥n
# Verificar que documentService.js use:
model: process.env.OLLAMA_EMBEDDING_MODEL || "nomic-embed-text"
```

#### 5. Rate Limit Exceeded

```bash
# S√≠ntomas
‚ùå Too Many Requests: Rate limit exceeded

# Soluciones
# Esperar 15 minutos o ajustar configuraci√≥n
RATE_LIMIT_MAX_REQUESTS=200
RATE_LIMIT_WINDOW_MS=900000
```

#### 6. Out of Memory (OOM)

```bash
# S√≠ntomas
‚ùå JavaScript heap out of memory

# Soluciones
# Aumentar memoria Node.js
node --max-old-space-size=4096 src/server.js

# Reducir chunk size
CHUNK_SIZE=500
CHUNK_OVERLAP=100
```

### Debugging Paso a Paso

#### 1. Verificar Estado del Sistema

```bash
# Health check general
curl http://localhost:3001/api/health

# Estado detallado
curl http://localhost:3001/api/health/detailed

# Diagn√≥stico RAG completo
curl http://localhost:3001/api/chat/diagnose
```

#### 2. Revisar Logs

```bash
# Logs en tiempo real
tail -f logs/app.log

# Solo errores
tail -f logs/error.log

# Buscar errores espec√≠ficos
grep "ERROR" logs/app.log
grep "Vector store" logs/app.log
```

#### 3. Validar Configuraci√≥n

```bash
# Verificar variables de entorno
cat .env

# Verificar modelos Ollama
ollama list

# Verificar archivos PDF
ls -la data/pdfs/

# Verificar vector store
ls -la data/vector-store/
```

#### 4. Testing Incremental

```bash
# 1. Test b√°sico de servidor
curl http://localhost:3001/api/health

# 2. Test Ollama
curl http://localhost:3001/api/chat/diagnose

# 3. Test chat simple
curl -X POST http://localhost:3001/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "hola"}'

# 4. Test RAG espec√≠fico
curl -X POST http://localhost:3001/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "¬øcu√°nto cuesta la leche?"}'
```

### Configuraci√≥n de Performance

#### Para Desarrollo (Recursos Limitados)

```env
# Modelo m√°s peque√±o
OLLAMA_MODEL=llama3.2:1b

# Chunks m√°s peque√±os
CHUNK_SIZE=500
CHUNK_OVERLAP=100

# Menos documentos en retrieval
RETRIEVAL_K=3

# Rate limiting m√°s restrictivo
RATE_LIMIT_MAX_REQUESTS=50
```

#### Para Producci√≥n (Recursos Abundantes)

```env
# Modelo m√°s grande
OLLAMA_MODEL=llama3.2:3b

# Chunks m√°s grandes para mejor contexto
CHUNK_SIZE=1500
CHUNK_OVERLAP=300

# M√°s documentos en retrieval
RETRIEVAL_K=7

# Rate limiting m√°s permisivo
RATE_LIMIT_MAX_REQUESTS=200
```
