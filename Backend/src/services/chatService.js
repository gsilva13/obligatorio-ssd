const { ChatOllama } = require("@langchain/ollama");
const { FaissStore } = require("@langchain/community/vectorstores/faiss");
const { OllamaEmbeddings } = require("@langchain/community/embeddings/ollama");
const { PromptTemplate } = require("@langchain/core/prompts");
const {
  RunnableSequence,
  RunnablePassthrough,
} = require("@langchain/core/runnables");
const { StringOutputParser } = require("@langchain/core/output_parsers");
const logger = require("../utils/logger");
const sessionService = require("./sessionService");
const path = require("path");

class ChatService {
  constructor() {
    this.ollama = null;
    this.vectorStore = null;
    this.embeddings = null;
    this.chain = null;
    this.initialized = false;
  }

  async initialize() {
    try {
      logger.info("üöÄ Inicializando ChatService...");

      // Configurar Ollama
      logger.debug("ü§ñ Configurando ChatOllama...");
      const ollamaConfig = {
        baseUrl: process.env.OLLAMA_BASE_URL || "http://127.0.0.1:11434",
        model: process.env.OLLAMA_MODEL || "llama3.2:1b",
        temperature: 0.7,
      };
      logger.debug("üìã Configuraci√≥n Ollama:", ollamaConfig);

      this.ollama = new ChatOllama(ollamaConfig);
      logger.info("‚úÖ ChatOllama configurado");

      // Configurar embeddings
      logger.debug("üßÆ Configurando OllamaEmbeddings...");
      const embeddingConfig = {
        baseUrl: process.env.OLLAMA_BASE_URL || "http://127.0.0.1:11434",
        model: process.env.OLLAMA_EMBEDDING_MODEL || "nomic-embed-text",
      };
      logger.debug("üìã Configuraci√≥n Embeddings:", embeddingConfig);

      this.embeddings = new OllamaEmbeddings(embeddingConfig);
      logger.info("‚úÖ OllamaEmbeddings configurado");

      // Probar conexi√≥n con Ollama antes de continuar
      logger.debug("üîó Probando conexi√≥n con Ollama...");
      try {
        const healthCheck = await this.checkOllamaHealth();
        logger.debug("üè• Estado de Ollama:", healthCheck);
        if (healthCheck.status !== "connected") {
          throw new Error(`Ollama no est√° disponible: ${healthCheck.error}`);
        }
        logger.info("‚úÖ Conexi√≥n con Ollama verificada");
      } catch (healthError) {
        logger.error("‚ùå Error verificando conexi√≥n Ollama:", healthError);
        throw healthError;
      }

      // Probar embeddings antes de cargar vector store
      logger.debug("üß™ Probando funcionalidad de embeddings...");
      try {
        const testEmbedding = await this.embeddings.embedQuery("test");
        logger.debug(
          `‚úÖ Embeddings funcionando, dimensiones: ${testEmbedding.length}`
        );
      } catch (embeddingError) {
        logger.error("‚ùå Error en embeddings:", embeddingError);
        throw new Error(`Embeddings no funcionan: ${embeddingError.message}`);
      }

      // Cargar vector store
      logger.debug("üìö Cargando vector store...");
      await this.loadVectorStore();

      // Configurar cadena RAG (mantener la original por ahora)
      logger.debug("‚õìÔ∏è  Configurando cadena RAG...");
      await this.setupRAGChain();

      this.initialized = true;
      logger.info("üéâ ChatService inicializado correctamente");

      // Log de resumen
      logger.info("üìä Resumen de inicializaci√≥n:");
      logger.info(
        `  - Ollama: ${ollamaConfig.model} en ${ollamaConfig.baseUrl}`
      );
      logger.info(`  - Embeddings: ${embeddingConfig.model}`);
      logger.info(
        `  - Vector Store: ${this.vectorStore ? "CARGADO" : "ERROR"}`
      );
    } catch (error) {
      logger.error("üí• Error cr√≠tico inicializando ChatService:", error);
      this.initialized = false;
      throw error;
    }
  }

  async loadVectorStore() {
    try {
      const vectorStorePath =
        process.env.VECTOR_STORE_PATH || "./data/vector-store";
      logger.info(
        `üìÇ Intentando cargar vector store desde: ${vectorStorePath}`
      );

      // Intentar cargar vector store existente
      try {
        logger.debug("üîÑ Cargando vector store existente...");
        this.vectorStore = await FaissStore.load(
          vectorStorePath,
          this.embeddings
        );
        logger.info("‚úÖ Vector store cargado desde archivo exitosamente");

        // Verificar que el vector store tiene contenido real
        const testSearch = await this.vectorStore.similaritySearch("test", 1);
        logger.info(
          `üìä Vector store contiene ${testSearch.length} documentos de prueba`
        );

        if (testSearch.length > 0) {
          logger.debug(
            `üìñ Primer documento: ${testSearch[0].pageContent.substring(
              0,
              100
            )}...`
          );
          logger.debug(
            `üè∑Ô∏è  Metadata: ${JSON.stringify(testSearch[0].metadata)}`
          );
        }
      } catch (error) {
        logger.error("‚ùå Error COMPLETO cargando vector store existente:", {
          message: error.message,
          stack: error.stack,
          name: error.name,
          code: error.code,
        });
        logger.warn(
          "‚ö†Ô∏è  No se pudo cargar vector store existente, se necesita crear uno nuevo"
        );
        logger.warn("üí° Ejecuta: npm run setup-vector-store");

        // Crear un vector store vac√≠o temporal
        logger.debug("üîÑ Creando vector store temporal vac√≠o...");
        this.vectorStore = await FaissStore.fromTexts(
          [
            "Informaci√≥n no disponible. Por favor, carga los documentos primero.",
          ],
          [{ source: "placeholder" }],
          this.embeddings
        );
        logger.warn("‚ö†Ô∏è  Vector store temporal creado (sin documentos reales)");
      }
    } catch (error) {
      logger.error("üí• Error cr√≠tico cargando vector store:", error);
      throw error;
    }
  }

  async setupRAGChain() {
    const retriever = this.vectorStore.asRetriever({
      k: 5, // N√∫mero de documentos relevantes a recuperar
      searchType: "similarity",
    });

    const promptTemplate = PromptTemplate.fromTemplate(`
Eres un asistente de Tienda Alemana, una cadena de supermercados. Tu trabajo es ayudar a los clientes respondiendo preguntas sobre:
- Productos disponibles, precios y stock
- Ubicaci√≥n de productos dentro de la tienda (g√≥ndolas y secciones)
- Informaci√≥n de sucursales (horarios, direcciones)

Contexto relevante de documentos:
{context}

{conversationHistory}

Pregunta actual del cliente: {question}

Instrucciones:
- Responde de manera amigable y profesional
- Mant√©n coherencia con la conversaci√≥n anterior si existe
- Si la informaci√≥n no est√° disponible en el contexto, dilo claramente
- Para preguntas sobre precios, incluye el s√≠mbolo $ y el valor exacto
- Para ubicaciones, menciona la g√≥ndola o secci√≥n espec√≠fica
- Para horarios, menciona d√≠as y horas completas
- Si el cliente hace referencia a algo mencionado anteriormente, √∫salo como contexto
- Si no puedes responder completamente, sugiere contactar con el personal de la tienda

Respuesta:`);

    // Funci√≥n para formatear documentos
    const formatDocs = (docs) => {
      return docs.map((doc) => doc.pageContent).join("\n\n");
    };

    // Crear la cadena RAG con contexto conversacional
    this.chain = RunnableSequence.from([
      {
        context: retriever.pipe(formatDocs),
        question: new RunnablePassthrough(),
        conversationHistory: (input) => {
          // Se espera que input tenga la forma: { question: "...", userId: "..." }
          if (input.userId) {
            const history = sessionService.getConversationContext(input.userId);
            return history ? `Historial de conversaci√≥n:\n${history}\n` : "";
          }
          return "";
        },
      },
      promptTemplate,
      this.ollama,
      new StringOutputParser(),
    ]);

    logger.info("Cadena RAG configurada correctamente");
  }

  async chat(message, userId = null) {
    if (!this.initialized) {
      throw new Error("ChatService no ha sido inicializado");
    }

    try {
      logger.info(
        `Procesando pregunta: ${message}${
          userId ? ` (Usuario: ${userId})` : ""
        }`
      );

      // Si no hay userId, usar uno temporal para esta conversaci√≥n
      const sessionId = userId || "anonymous_session";

      // Agregar mensaje del usuario al historial de sesi√≥n
      sessionService.addMessage(sessionId, message, null, true);

      // Enfoque h√≠brido: intentar RAG, si falla usar chat simple
      const response = await this.hybridChat(message, sessionId);

      // Agregar respuesta del asistente al historial de sesi√≥n
      sessionService.addMessage(sessionId, null, response.response, false);

      logger.info("Respuesta generada correctamente");
      return {
        success: true,
        response: response.response,
        userId: sessionId,
        timestamp: new Date().toISOString(),
        ragUsed: response.ragUsed,
      };
    } catch (error) {
      logger.error("Error en chat:", error);

      // Fallback final: chat simple sin contexto
      try {
        const fallbackResponse = await this.simpleChatFallback(message);
        return {
          success: true,
          response: fallbackResponse,
          userId: sessionId || "anonymous_session",
          timestamp: new Date().toISOString(),
          ragUsed: false,
          fallback: true,
        };
      } catch (fallbackError) {
        logger.error("Error en fallback:", fallbackError);
        return {
          success: false,
          error: "Error procesando la consulta",
          message: error.message,
          timestamp: new Date().toISOString(),
        };
      }
    }
  }

  async hybridChat(message, sessionId) {
    logger.debug(`üîç Iniciando hybridChat para sessionId: ${sessionId}`);

    // Obtener contexto conversacional
    const history = sessionService.getConversationContext(sessionId);
    logger.debug(
      `üí≠ Contexto conversacional: ${history ? "DISPONIBLE" : "VAC√çO"}`
    );
    if (history) {
      logger.debug(`üìù Historial: ${history.substring(0, 100)}...`);
    }

    // Intentar obtener documentos relevantes con RAG
    let ragContext = "";
    let ragUsed = false;

    try {
      logger.info(`üîç Iniciando b√∫squeda RAG para: "${message}"`);

      // Verificar estado del vector store
      if (!this.vectorStore) {
        throw new Error("Vector store no inicializado");
      }
      logger.debug("‚úÖ Vector store disponible");

      // Verificar embeddings
      if (!this.embeddings) {
        throw new Error("Embeddings no inicializados");
      }
      logger.debug("‚úÖ Embeddings disponibles");

      // Probar embedding del mensaje antes de b√∫squeda
      logger.debug("üßÆ Probando generar embedding del mensaje...");
      try {
        const testEmbedding = await this.embeddings.embedQuery(message);
        logger.debug(
          `‚úÖ Embedding generado exitosamente, dimensiones: ${testEmbedding.length}`
        );
      } catch (embError) {
        logger.error("‚ùå Error generando embedding del mensaje:", embError);
        throw embError;
      }

      // Realizar b√∫squeda de similitud
      logger.debug("üîç Ejecutando similaritySearch...");
      const docs = await this.vectorStore.similaritySearch(message, 3);

      logger.info(`üìÑ Documentos encontrados: ${docs.length}`);

      if (docs.length === 0) {
        logger.warn("‚ö†Ô∏è  No se encontraron documentos relevantes");
        ragContext =
          "No se encontraron documentos espec√≠ficos para esta consulta.";
      } else {
        ragContext = docs
          .map((doc, index) => {
            logger.debug(
              `üìñ Doc ${index + 1}: ${doc.pageContent.substring(0, 100)}...`
            );
            logger.debug(`üè∑Ô∏è  Metadata: ${JSON.stringify(doc.metadata)}`);
            return doc.pageContent;
          })
          .join("\n\n");

        logger.info(
          `üìö Contexto RAG generado: ${ragContext.length} caracteres`
        );
        ragUsed = true;
      }
    } catch (ragError) {
      logger.error("‚ùå Error detallado en RAG:", {
        message: ragError.message,
        stack: ragError.stack,
        name: ragError.name,
      });

      // Logs adicionales para debugging
      logger.debug("üîß Estado de componentes:");
      logger.debug(
        `  - Vector store: ${this.vectorStore ? "INICIALIZADO" : "NULL"}`
      );
      logger.debug(
        `  - Embeddings: ${this.embeddings ? "INICIALIZADO" : "NULL"}`
      );
      logger.debug(`  - Mensaje: "${message}"`);

      ragContext =
        "Informaci√≥n espec√≠fica de documentos no disponible en este momento.";
      ragUsed = false;
    }

    // Crear prompt h√≠brido con contexto conversacional + RAG (si disponible)
    logger.debug("üìù Generando prompt con contexto h√≠brido...");
    const promptTemplate = PromptTemplate.fromTemplate(`
Eres un asistente de Tienda Alemana, una cadena de supermercados. Tu trabajo es ayudar a los clientes respondiendo preguntas sobre:
- Productos disponibles, precios y stock
- Ubicaci√≥n de productos dentro de la tienda (g√≥ndolas y secciones)
- Informaci√≥n de sucursales (horarios, direcciones)

Informaci√≥n de documentos:
{context}

{conversationHistory}

Pregunta actual del cliente: {question}

Instrucciones:
- Responde de manera amigable y profesional
- Mant√©n coherencia con la conversaci√≥n anterior si existe
- Si la informaci√≥n espec√≠fica no est√° disponible en los documentos, usa tu conocimiento general sobre supermercados
- Para preguntas sobre precios, incluye el s√≠mbolo $ y proporciona rangos realistas
- Para ubicaciones, menciona secciones t√≠picas de supermercado
- Si el cliente hace referencia a algo mencionado anteriormente, √∫salo como contexto
- Si no puedes responder completamente, sugiere contactar con el personal de la tienda

Respuesta:`);

    try {
      // Generar respuesta
      logger.debug("ü§ñ Enviando prompt a Ollama...");
      const chain = promptTemplate
        .pipe(this.ollama)
        .pipe(new StringOutputParser());

      const promptData = {
        context: ragContext,
        conversationHistory: history
          ? `Historial de conversaci√≥n:\n${history}\n`
          : "",
        question: message,
      };

      logger.debug("üì§ Datos del prompt:", {
        contextLength: ragContext.length,
        hasHistory: !!history,
        question: message,
      });

      const response = await chain.invoke(promptData);
      logger.info("‚úÖ Respuesta generada exitosamente");
      logger.debug(`üìù Respuesta: ${response.substring(0, 100)}...`);

      return {
        response: response.trim(),
        ragUsed: ragUsed,
      };
    } catch (ollamaError) {
      logger.error(
        "‚ùå Error en generaci√≥n de respuesta con Ollama:",
        ollamaError
      );
      throw ollamaError;
    }
  }

  async simpleChatFallback(message) {
    logger.debug("Usando fallback de chat simple");

    const simplePrompt = PromptTemplate.fromTemplate(`
Eres un asistente de Tienda Alemana. Responde de manera amigable y profesional.

Pregunta: {question}

Respuesta breve y √∫til:`);

    const chain = simplePrompt.pipe(this.ollama).pipe(new StringOutputParser());
    const response = await chain.invoke({ question: message });

    return response.trim();
  }

  async getRelevantDocuments(query, k = 5) {
    if (!this.vectorStore) {
      throw new Error("Vector store no inicializado");
    }

    try {
      logger.debug(`üîç Buscando documentos relevantes para: "${query}"`);
      logger.debug(`üìä N√∫mero de documentos solicitados: ${k}`);

      const docs = await this.vectorStore.similaritySearch(query, k);

      logger.info(`üìÑ Documentos encontrados: ${docs.length}`);
      docs.forEach((doc, index) => {
        logger.debug(`üìñ Doc ${index + 1}:`, {
          content: doc.pageContent.substring(0, 100) + "...",
          metadata: doc.metadata,
          contentLength: doc.pageContent.length,
        });
      });

      return docs.map((doc) => ({
        content: doc.pageContent,
        metadata: doc.metadata,
        score: doc.score || 0,
      }));
    } catch (error) {
      logger.error("‚ùå Error obteniendo documentos relevantes:", error);
      throw error;
    }
  }

  // M√©todo de diagn√≥stico para RAG
  async diagnoseRAG() {
    logger.info("üîß Iniciando diagn√≥stico completo de RAG...");

    const diagnosis = {
      timestamp: new Date().toISOString(),
      initialized: this.initialized,
      components: {},
      tests: {},
    };

    try {
      // Verificar componentes
      diagnosis.components.ollama = !!this.ollama;
      diagnosis.components.embeddings = !!this.embeddings;
      diagnosis.components.vectorStore = !!this.vectorStore;
      diagnosis.components.chain = !!this.chain;

      logger.debug("üìã Estado de componentes:", diagnosis.components);

      // Test 1: Conexi√≥n Ollama
      try {
        const health = await this.checkOllamaHealth();
        diagnosis.tests.ollamaConnection = { success: true, data: health };
        logger.debug("‚úÖ Test Ollama: PASSED");
      } catch (error) {
        diagnosis.tests.ollamaConnection = {
          success: false,
          error: error.message,
        };
        logger.error("‚ùå Test Ollama: FAILED", error.message);
      }

      // Test 2: Embeddings
      try {
        const testEmbedding = await this.embeddings.embedQuery("test query");
        diagnosis.tests.embeddings = {
          success: true,
          dimensions: testEmbedding.length,
          sampleValues: testEmbedding.slice(0, 5),
        };
        logger.debug(
          `‚úÖ Test Embeddings: PASSED (${testEmbedding.length} dims)`
        );
      } catch (error) {
        diagnosis.tests.embeddings = { success: false, error: error.message };
        logger.error("‚ùå Test Embeddings: FAILED", error.message);
      }

      // Test 3: Vector Store Search
      try {
        const docs = await this.vectorStore.similaritySearch("test", 2);
        diagnosis.tests.vectorSearch = {
          success: true,
          documentsFound: docs.length,
          sampleContent: docs.map((d) => d.pageContent.substring(0, 50)),
        };
        logger.debug(`‚úÖ Test Vector Search: PASSED (${docs.length} docs)`);
      } catch (error) {
        diagnosis.tests.vectorSearch = { success: false, error: error.message };
        logger.error("‚ùå Test Vector Search: FAILED", error.message);
      }

      // Test 4: End-to-end RAG
      try {
        const ragResult = await this.hybridChat(
          "¬øcu√°les son los precios?",
          "diagnostic_test"
        );
        diagnosis.tests.endToEndRAG = {
          success: true,
          ragUsed: ragResult.ragUsed,
          responseLength: ragResult.response.length,
        };
        logger.debug(
          `‚úÖ Test End-to-End RAG: PASSED (RAG Used: ${ragResult.ragUsed})`
        );
      } catch (error) {
        diagnosis.tests.endToEndRAG = { success: false, error: error.message };
        logger.error("‚ùå Test End-to-End RAG: FAILED", error.message);
      }

      // Resumen
      const passedTests = Object.values(diagnosis.tests).filter(
        (t) => t.success
      ).length;
      const totalTests = Object.keys(diagnosis.tests).length;

      logger.info(
        `üéØ Diagn√≥stico completado: ${passedTests}/${totalTests} tests passed`
      );

      return diagnosis;
    } catch (error) {
      logger.error("üí• Error en diagn√≥stico:", error);
      diagnosis.tests.diagnostic = { success: false, error: error.message };
      return diagnosis;
    }
  }

  // M√©todo para verificar si Ollama est√° disponible
  async checkOllamaHealth() {
    try {
      // Usar fetch directo para verificar la conexi√≥n con la URL correcta
      const baseUrl = process.env.OLLAMA_BASE_URL || "http://127.0.0.1:11434";

      const response = await fetch(`${baseUrl}/api/version`);
      if (response.ok) {
        const data = await response.json();
        return {
          status: "connected",
          model: process.env.OLLAMA_MODEL,
          version: data.version,
        };
      } else {
        throw new Error(`HTTP ${response.status}`);
      }
    } catch (error) {
      logger.error("Error conectando con Ollama:", error);
      return { status: "disconnected", error: error.message };
    }
  }
}

module.exports = new ChatService();
